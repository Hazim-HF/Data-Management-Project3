{"paragraphs":[{"text":"%md\n# Exploring MovieLens 100k Dataset using Cassandra and Spark\nProject 3 Data Management STQD6324\nHazim Fitri Bin Ahmad Faudzi (P152419)","user":"anonymous","dateUpdated":"2025-07-23T22:42:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Exploring MovieLens 100k Dataset using Cassandra and Spark</h1>\n<p>Project 3 Data Management STQD6324\n<br  />Hazim Fitri Bin Ahmad Faudzi (P152419)</p>\n"}]},"apps":[],"jobName":"paragraph_1753226768191_-1438873513","id":"20250722-232608_226798370","dateCreated":"2025-07-22T23:26:08+0000","dateStarted":"2025-07-23T22:42:12+0000","dateFinished":"2025-07-23T22:42:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5315"},{"text":"%md\n**In this project, we're going to explore MovieLens 100k Dataset using Apache Spark and Cassandra. To run this, make sure `Spark 2` and `Cassandra` are running.**\n\n**The first step we need to do is download 3 data (u.data, u.item, u.user) into our local computer.**","user":"anonymous","dateUpdated":"2025-07-23T22:42:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><strong>In this project, we're going to explore MovieLens 100k Dataset using Apache Spark and Cassandra. To run this, make sure <code>Spark 2</code> and <code>Cassandra</code> are running.</strong></p>\n<p><strong>The first step we need to do is download 3 data (u.data, u.item, u.user) into our local computer.</strong></p>\n"}]},"apps":[],"jobName":"paragraph_1753227044280_-1768604965","id":"20250722-233044_1247454924","dateCreated":"2025-07-22T23:30:44+0000","dateStarted":"2025-07-23T22:42:12+0000","dateFinished":"2025-07-23T22:42:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5316"},{"text":"%sh\nwget http://media.sundog-soft.com/hadoop/ml-100k/u.data -O /tmp/u.data\nwget http://media.sundog-soft.com/hadoop/ml-100k/u.item -O /tmp/u.item\nwget http://media.sundog-soft.com/hadoop/ml-100k/u.user -O /tmp/u.user","user":"anonymous","dateUpdated":"2025-07-23T22:42:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"--2025-07-23 22:42:12--  http://media.sundog-soft.com/hadoop/ml-100k/u.data\nResolving media.sundog-soft.com (media.sundog-soft.com)... 3.5.17.89, 3.5.10.67, 16.182.74.121, ...\nConnecting to media.sundog-soft.com (media.sundog-soft.com)|3.5.17.89|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2079229 (2.0M) [application/octet-stream]\nSaving to: ‘/tmp/u.data’\n\n     0K .......... .......... .......... .......... ..........  2% 97.1K 20s\n    50K .......... .......... .......... .......... ..........  4%  205K 15s\n   100K .......... .......... .......... .......... ..........  7% 1.67M 10s\n   150K .......... .......... .......... .......... ..........  9%  247K 9s\n   200K .......... .......... .......... .......... .......... 12% 1.11M 7s\n   250K .......... .......... .......... .......... .......... 14% 3.07M 6s\n   300K .......... .......... .......... .......... .......... 17% 10.8M 5s\n   350K .......... .......... .......... .......... .......... 19%  273K 5s\n   400K .......... .......... .......... .......... .......... 22% 1.68M 4s\n   450K .......... .......... .......... .......... .......... 24% 13.1M 4s\n   500K .......... .......... .......... .......... .......... 27% 1.45M 4s\n   550K .......... .......... .......... .......... .......... 29% 29.9M 3s\n   600K .......... .......... .......... .......... .......... 32% 3.45M 3s\n   650K .......... .......... .......... .......... .......... 34% 30.3M 3s\n   700K .......... .......... .......... .......... .......... 36% 11.2M 2s\n   750K .......... .......... .......... .......... .......... 39%  292K 2s\n   800K .......... .......... .......... .......... .......... 41% 3.36M 2s\n   850K .......... .......... .......... .......... .......... 44% 2.36M 2s\n   900K .......... .......... .......... .......... .......... 46% 3.20M 2s\n   950K .......... .......... .......... .......... .......... 49% 9.46M 2s\n  1000K .......... .......... .......... .......... .......... 51% 2.99M 1s\n  1050K .......... .......... .......... .......... .......... 54% 7.15M 1s\n  1100K .......... .......... .......... .......... .......... 56% 10.4M 1s\n  1150K .......... .......... .......... .......... .......... 59% 3.46M 1s\n  1200K .......... .......... .......... .......... .......... 61% 3.43M 1s\n  1250K .......... .......... .......... .......... .......... 64% 20.7M 1s\n  1300K .......... .......... .......... .......... .......... 66% 2.93M 1s\n  1350K .......... .......... .......... .......... .......... 68% 6.24M 1s\n  1400K .......... .......... .......... .......... .......... 71% 7.73M 1s\n  1450K .......... .......... .......... .......... .......... 73% 2.97M 1s\n  1500K .......... .......... .......... .......... .......... 76%  538K 1s\n  1550K .......... .......... .......... .......... .......... 78% 5.81M 0s\n  1600K .......... .......... .......... .......... .......... 81% 3.40M 0s\n  1650K .......... .......... .......... .......... .......... 83% 2.32M 0s\n  1700K .......... .......... .......... .......... .......... 86% 4.39M 0s\n  1750K .......... .......... .......... .......... .......... 88% 29.7M 0s\n  1800K .......... .......... .......... .......... .......... 91% 4.86M 0s\n  1850K .......... .......... .......... .......... .......... 93% 2.89M 0s\n  1900K .......... .......... .......... .......... .......... 96% 9.73M 0s\n  1950K .......... .......... .......... .......... .......... 98% 4.55M 0s\n  2000K .......... .......... ..........                      100% 16.4M=1.9s\n\n2025-07-23 22:42:15 (1.07 MB/s) - ‘/tmp/u.data’ saved [2079229/2079229]\n\n--2025-07-23 22:42:15--  http://media.sundog-soft.com/hadoop/ml-100k/u.item\nResolving media.sundog-soft.com (media.sundog-soft.com)... 3.5.17.89, 3.5.10.67, 16.182.74.121, ...\nConnecting to media.sundog-soft.com (media.sundog-soft.com)|3.5.17.89|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 236344 (231K) [application/octet-stream]\nSaving to: ‘/tmp/u.item’\n\n     0K .......... .......... .......... .......... .......... 21% 97.9K 2s\n    50K .......... .......... .......... .......... .......... 43%  206K 1s\n   100K .......... .......... .......... .......... .......... 64% 3.80M 0s\n   150K .......... .......... .......... .......... .......... 86%  193K 0s\n   200K .......... .......... ..........                      100% 22.3M=1.0s\n\n2025-07-23 22:42:16 (225 KB/s) - ‘/tmp/u.item’ saved [236344/236344]\n\n--2025-07-23 22:42:16--  http://media.sundog-soft.com/hadoop/ml-100k/u.user\nResolving media.sundog-soft.com (media.sundog-soft.com)... 52.217.121.145, 3.5.1.117, 16.15.193.79, ...\nConnecting to media.sundog-soft.com (media.sundog-soft.com)|52.217.121.145|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 22628 (22K) [text/plain]\nSaving to: ‘/tmp/u.user’\n\n     0K .......... .......... ..                              100% 88.0K=0.3s\n\n2025-07-23 22:42:17 (88.0 KB/s) - ‘/tmp/u.user’ saved [22628/22628]\n\n"}]},"apps":[],"jobName":"paragraph_1753305139240_76806849","id":"20250723-211219_194657582","dateCreated":"2025-07-23T21:12:19+0000","dateStarted":"2025-07-23T22:42:12+0000","dateFinished":"2025-07-23T22:42:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5317"},{"text":"%md\n**Remove the directory in Hadoop if already existed and recreate the directory to avoid conflicts with old files if exists and ensure clean state.**\n","user":"anonymous","dateUpdated":"2025-07-23T22:42:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><strong>Remove the directory in Hadoop if already existed and recreate the directory to avoid conflicts with old files if exists and ensure clean state.</strong></p>\n"}]},"apps":[],"jobName":"paragraph_1753305715895_2126207709","id":"20250723-212155_2065671377","dateCreated":"2025-07-23T21:21:55+0000","dateStarted":"2025-07-23T22:42:12+0000","dateFinished":"2025-07-23T22:42:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5318"},{"text":"%sh\nhdfs dfs -rm -r -f /tmp/ml-100k\nhdfs dfs -mkdir /tmp/ml-100k","user":"anonymous","dateUpdated":"2025-07-23T22:42:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"25/07/23 22:42:14 INFO fs.TrashPolicyDefault: Moved: 'hdfs://sandbox-hdp.hortonworks.com:8020/tmp/ml-100k' to trash at: hdfs://sandbox-hdp.hortonworks.com:8020/user/zeppelin/.Trash/Current/tmp/ml-100k1753310534263\n"}]},"apps":[],"jobName":"paragraph_1753305607342_1573538078","id":"20250723-212007_1539263063","dateCreated":"2025-07-23T21:20:07+0000","dateStarted":"2025-07-23T22:42:12+0000","dateFinished":"2025-07-23T22:42:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5319"},{"text":"%md\n**Next, put the data that we've donwloaded into Hadoop directory that we've just created**\n","user":"anonymous","dateUpdated":"2025-07-23T22:42:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><strong>Next, put the data that we've donwloaded into Hadoop directory that we've just created</strong></p>\n"}]},"apps":[],"jobName":"paragraph_1753306044264_-684507244","id":"20250723-212724_1933804888","dateCreated":"2025-07-23T21:27:24+0000","dateStarted":"2025-07-23T22:42:12+0000","dateFinished":"2025-07-23T22:42:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5320"},{"text":"%sh\nhadoop fs -put /tmp/u.data /tmp/ml-100k/\nhadoop fs -put /tmp/u.item /tmp/ml-100k/\nhadoop fs -put /tmp/u.user /tmp/ml-100k/","user":"anonymous","dateUpdated":"2025-07-23T22:42:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"put: `/tmp/ml-100k/': No such file or directory: `hdfs://sandbox-hdp.hortonworks.com:8020/tmp/ml-100k'\n"}]},"apps":[],"jobName":"paragraph_1753306079064_399376380","id":"20250723-212759_2036765557","dateCreated":"2025-07-23T21:27:59+0000","dateStarted":"2025-07-23T22:42:13+0000","dateFinished":"2025-07-23T22:42:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5321"},{"text":"%md\n**Create Resilient Distributed Dataset (RDD) in Spark for further analysis using Apache Spark.**\n","user":"anonymous","dateUpdated":"2025-07-23T22:42:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><strong>Create Resilient Distributed Dataset (RDD) in Spark for further analysis using Apache Spark.</strong></p>\n"}]},"apps":[],"jobName":"paragraph_1753306171664_1917144037","id":"20250723-212931_1436378044","dateCreated":"2025-07-23T21:29:31+0000","dateStarted":"2025-07-23T22:42:12+0000","dateFinished":"2025-07-23T22:42:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5322"},{"text":"%md\n**1. Parse `u.data` and Store Movie Ratings to Cassandra (Table: `ratings`)**\n","user":"anonymous","dateUpdated":"2025-07-23T22:42:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><strong>1. Parse <code>u.data</code> and Store Movie Ratings to Cassandra (Table: <code>ratings</code>)</strong></p>\n"}]},"apps":[],"jobName":"paragraph_1753306314045_-1967849498","id":"20250723-213154_1031245865","dateCreated":"2025-07-23T21:31:54+0000","dateStarted":"2025-07-23T22:42:12+0000","dateFinished":"2025-07-23T22:42:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5323"},{"text":"%pyspark\n\n# import necessary PySpark libraries\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions as F\n\n# define a function to parse each line of the ratings data\ndef parse_ratings(line):\n    fields = line.split('\\t')   # split each line by tab character\n    return Row(\n        user_id=int(fields[0]), # extract user ID as integer\n        movie_id=int(fields[1]),# extract movie ID as integer\n        rating=int(fields[2]))  # extract rating as integer\n\n# read the ratings data file from HDFS\nratings_lines = spark.sparkContext.textFile(\"hdfs:///tmp/ml-100k/u.data\")\n\n# map each line using parse_ratings function to create an RDD of Row objects\nratings = ratings_lines.map(parse_ratings)\n\n# convert the RDD of rows into a dataframe\nratingsDataset = spark.createDataFrame(ratings)\n\n# write the dataframe to the cassandra table 'ratigs' in the 'movielens' keyspace(database)\nratingsDataset.write \\\n    .format(\"org.apache.spark.sql.cassandra\") \\\n    .mode(\"append\") \\\n    .options(table=\"ratings\", keyspace=\"movielens\") \\\n    .save()\n\n# read the 'ratings' table back from Cassandra into a dataframe\nreadRatings = spark.read \\\n    .format(\"org.apache.spark.sql.cassandra\") \\\n    .options(table=\"ratings\", keyspace=\"movielens\") \\\n    .load()\n\n# display the contents of the dataframe\nreadRatings.show()","user":"anonymous","dateUpdated":"2025-07-23T22:42:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1753306394005_-815637100","id":"20250723-213314_1315017004","dateCreated":"2025-07-23T21:33:14+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:5324","dateFinished":"2025-07-23T22:42:14+0000","dateStarted":"2025-07-23T22:42:12+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8990687735966586171.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8990687735966586171.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 15, in <module>\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/readwriter.py\", line 703, in save\n    self._jwrite.save()\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\", line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\", line 320, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o1079.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 105.0 failed 4 times, most recent failure: Lost task 1.3 in stage 105.0 (TID 9981, sandbox-hdp.hortonworks.com, executor 1): java.io.FileNotFoundException: File does not exist: /tmp/ml-100k/u.data\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:2029)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2000)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1913)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:700)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:377)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1251)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1236)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1224)\n\tat org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:309)\n\tat org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:274)\n\tat org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:266)\n\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1549)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:332)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:327)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:340)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:257)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:256)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:214)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /tmp/ml-100k/u.data\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:2029)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2000)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1913)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:700)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:377)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1498)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1398)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n\tat com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:290)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:202)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:184)\n\tat com.sun.proxy.$Proxy16.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1249)\n\t... 49 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat com.datastax.spark.connector.RDDFunctions.saveToCassandra(RDDFunctions.scala:36)\n\tat org.apache.spark.sql.cassandra.CassandraSourceRelation.insert(CassandraSourceRelation.scala:80)\n\tat org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:86)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: File does not exist: /tmp/ml-100k/u.data\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:2029)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2000)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1913)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:700)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:377)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1251)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1236)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1224)\n\tat org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:309)\n\tat org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:274)\n\tat org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:266)\n\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1549)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:332)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:327)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:340)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:257)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:256)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:214)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /tmp/ml-100k/u.data\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:2029)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:2000)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1913)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:700)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:377)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1498)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1398)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n\tat com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:290)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:202)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:184)\n\tat com.sun.proxy.$Proxy16.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1249)\n\t... 49 more\n\n\n"}]}},{"text":"%md\n**2. Parse `u.item` and Store Movie Ratings to Cassandra (Table: `titles`)**\n","user":"anonymous","dateUpdated":"2025-07-23T22:42:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><strong>2. Parse <code>u.item</code> and Store Movie Ratings to Cassandra (Table: <code>titles</code>)</strong></p>\n"}]},"apps":[],"jobName":"paragraph_1753306852300_-97218959","id":"20250723-214052_1231787327","dateCreated":"2025-07-23T21:40:52+0000","dateStarted":"2025-07-23T22:42:12+0000","dateFinished":"2025-07-23T22:42:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5325"},{"text":"%pyspark\n\n# import necessary PySpark libraries \nfrom pyspark.sql import Row\nfrom pyspark.sql import functions as F\n\n# define a function to parse each line from the movie metadata file\ndef parse_titles(line):\n    fields = line.split('|')        # split line using '|' delimiter\n    return Row(\n        movie_id=int(fields[0]),    # unique movie ID\n        title=fields[1],            # movie title\n        unknown=int(fields[5]),     # genre binary flags: 1 = belongs to genre, 0 = not\n        action=int(fields[6]),\n        adventure=int(fields[7]),\n        animation=int(fields[8]),\n        children=int(fields[9]),\n        comedy=int(fields[10]),\n        crime=int(fields[11]),\n        documentary=int(fields[12]),\n        drama=int(fields[13]),\n        fantasy=int(fields[14]),\n        filmnoir=int(fields[15]),\n        horror=int(fields[16]),\n        musical=int(fields[17]),\n        mystery=int(fields[18]),\n        romance=int(fields[19]),\n        scifi=int(fields[20]),\n        thriller=int(fields[21]),\n        war=int(fields[22]),\n        western=int(fields[23])\n    )\n\n# load  the raw movie data file from HDFS\nmovie_lines = spark.sparkContext.textFile(\"hdfs:///tmp/ml-100k/u.item\")\n\n# map each line to a row object using the parsing function\nmovies = movie_lines.map(parse_titles)\n\n# convert the RDD of rows into a spark dataframe\nmovies_df = spark.createDataFrame(movies)\n\n# write the dataframe into the cassandra table 'titles' in the 'movielens' keyspace(database)\nmovies_df.write \\\n    .format(\"org.apache.spark.sql.cassandra\") \\\n    .mode(\"append\") \\\n    .options(table=\"titles\", keyspace=\"movielens\") \\\n    .save()\n\n# read the 'titles' table back from the cassandra to verify successful write\nreadMovies = spark.read \\\n    .format(\"org.apache.spark.sql.cassandra\") \\\n    .options(table=\"titles\", keyspace=\"movielens\") \\\n    .load()\n\n# display the content of the 'titles' tables\nreadMovies.show()","user":"anonymous","dateUpdated":"2025-07-23T22:42:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1753306937208_2118439399","id":"20250723-214217_1089123165","dateCreated":"2025-07-23T21:42:17+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:5326","dateFinished":"2025-07-23T22:42:14+0000","dateStarted":"2025-07-23T22:42:13+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8990687735966586171.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8990687735966586171.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 30, in <module>\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 693, in createDataFrame\n    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 390, in _createFromRDD\n    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 361, in _inferSchema\n    first = rdd.first()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1376, in first\n    rs = self.take(1)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1328, in take\n    totalParts = self.getNumPartitions()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 2455, in getNumPartitions\n    return self._prev_jrdd.partitions().size()\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\", line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\", line 320, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o1134.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs:/tmp/ml-100k/u.item\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n"}]}},{"text":"%md\n**3. Parse `u.user` and Store Movie Ratings to Cassandra (Table: `users`)**","user":"anonymous","dateUpdated":"2025-07-23T22:42:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p><strong>3. Parse <code>u.user</code> and Store Movie Ratings to Cassandra (Table: <code>users</code>)</strong></p>\n"}]},"apps":[],"jobName":"paragraph_1753306935794_-1532458238","id":"20250723-214215_688934819","dateCreated":"2025-07-23T21:42:15+0000","dateStarted":"2025-07-23T22:42:12+0000","dateFinished":"2025-07-23T22:42:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5327"},{"text":"%pyspark\n\n# import necessary PySpark libraries\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions as F\n\n# defind a function to parse each line of the user data file\ndef parse_users(line):\n    fields = line.split('|')    # split the line by '|'\n    return Row(\n        user_id=int(fields[0]), # extract user ID as integer\n        age=int(fields[1]),     # extract age as integer\n        gender=fields[2],       # extract gender as string\n        occupation=fields[3],   # extract occupation as string\n        zip=fields[4])          # extract zip code as string\n\n# read the user data from HDFS\nusers_lines = spark.sparkContext.textFile(\"hdfs:///tmp/ml-100k/u.user\")\n\n# apply the parsing function to each line to create a RDD of row objects\nusers = users_lines.map(parse_users)\n\n# convert the RDD of row objects to a Spark dataframe\nusersDataset = spark.createDataFrame(users)\n\n# write the dataframe to a cassandra table named 'users' in the 'movielens' keyspace(database)\nusersDataset.write \\\n    .format(\"org.apache.spark.sql.cassandra\") \\\n    .mode(\"append\") \\\n    .options(table=\"users\", keyspace=\"movielens\") \\\n    .save()\n\n# read the data back from the Cassandra table into a new dataframe\nreadUsers = spark.read \\\n    .format(\"org.apache.spark.sql.cassandra\") \\\n    .options(table=\"users\", keyspace=\"movielens\") \\\n    .load()\n\n# show the content of the dataframe loaded from Cassandra\nreadUsers.show()","user":"anonymous","dateUpdated":"2025-07-23T22:42:14+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1753306925359_795657356","id":"20250723-214205_1445192140","dateCreated":"2025-07-23T21:42:05+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:5328","dateFinished":"2025-07-23T22:42:15+0000","dateStarted":"2025-07-23T22:42:14+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8990687735966586171.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8990687735966586171.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 13, in <module>\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 693, in createDataFrame\n    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 390, in _createFromRDD\n    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 361, in _inferSchema\n    first = rdd.first()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1376, in first\n    rs = self.take(1)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1328, in take\n    totalParts = self.getNumPartitions()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 2455, in getNumPartitions\n    return self._prev_jrdd.partitions().size()\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\", line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\", line 320, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o1165.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs:/tmp/ml-100k/u.user\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n"}]}},{"text":"%md\n### i) Calculate the average rating for each movie.","user":"anonymous","dateUpdated":"2025-07-23T22:42:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>i) Calculate the average rating for each movie.</h3>\n"}]},"apps":[],"jobName":"paragraph_1753226993182_1960103895","id":"20250722-232953_508550520","dateCreated":"2025-07-22T23:29:53+0000","dateStarted":"2025-07-23T22:42:13+0000","dateFinished":"2025-07-23T22:42:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5329"},{"text":"%pyspark\n\n# import necessary PySpark libarary\nfrom pyspark.sql.functions import avg\n\n# join the ratings and movie titles datafrae on the movie_id column\ncombine_data = readRatings.join(readMovies, on=\"movie_id\")\n\n# group by the movie title and calculate the average rating for each movie\navg_ratings = combine_data.groupBy(\"title\").agg(avg(\"rating\").alias(\"avg_rating\"))\n\n# show the  average ratings for each movies\navg_ratings.show()","user":"anonymous","dateUpdated":"2025-07-23T22:42:15+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1753307477372_1026291037","id":"20250723-215117_1359631894","dateCreated":"2025-07-23T21:51:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5330","dateFinished":"2025-07-23T22:42:17+0000","dateStarted":"2025-07-23T22:42:15+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+------------------+\n|               title|        avg_rating|\n+--------------------+------------------+\n|       Psycho (1960)| 4.100418410041841|\n|   Annie Hall (1977)| 3.911111111111111|\n|    Fair Game (1995)|2.1818181818181817|\n|Heavenly Creature...|3.6714285714285713|\n|Night of the Livi...|          3.421875|\n|         Cosi (1996)|               4.0|\n|When We Were King...| 4.045454545454546|\n|Paris, France (1993)|2.3333333333333335|\n| If Lucy Fell (1996)|2.7586206896551726|\n|Snow White and th...|3.7093023255813953|\n| Three Wishes (1995)|3.2222222222222223|\n|I'll Do Anything ...|               2.6|\n|Spanking the Monk...| 3.074074074074074|\n|        Mondo (1996)|               3.0|\n|A Chef in Love (1...|             4.125|\n|Last Action Hero ...|2.7457627118644066|\n|Colonel Chabert, ...|               3.5|\n| Evil Dead II (1987)|3.5168539325842696|\n|    Nico Icon (1995)|               4.0|\n|Reality Bites (1994)| 2.961038961038961|\n+--------------------+------------------+\nonly showing top 20 rows\n\n"}]}},{"text":"%md\n### ii) Identify the top ten movies with the highest average ratings.","user":"anonymous","dateUpdated":"2025-07-23T22:42:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>ii) Identify the top ten movies with the highest average ratings.</h3>\n"}]},"apps":[],"jobName":"paragraph_1753227015421_-595166792","id":"20250722-233015_1852857503","dateCreated":"2025-07-22T23:30:15+0000","dateStarted":"2025-07-23T22:42:13+0000","dateFinished":"2025-07-23T22:42:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5331"},{"text":"%pyspark\n# sort the movies by average rating in descending order\ntop10_ratings = avg_ratings_df.orderBy(\"avg_rating\", ascending=False).limit(10)\n\n# show the top 10 movies with highest average ratings\ntop10_ratings.show()","user":"anonymous","dateUpdated":"2025-07-23T22:42:15+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1753307784284_-1257333247","id":"20250723-215624_183903520","dateCreated":"2025-07-23T21:56:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6744","dateFinished":"2025-07-23T22:42:19+0000","dateStarted":"2025-07-23T22:42:15+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+----------+\n|               title|avg_rating|\n+--------------------+----------+\n|Someone Else's Am...|       5.0|\n|Saint of Fort Was...|       5.0|\n|Aiqing wansui (1994)|       5.0|\n|Marlene Dietrich:...|       5.0|\n|     Star Kid (1997)|       5.0|\n|Great Day in Harl...|       5.0|\n|Entertaining Ange...|       5.0|\n|They Made Me a Cr...|       5.0|\n|Santa with Muscle...|       5.0|\n|  Prefontaine (1997)|       5.0|\n+--------------------+----------+\n\n"}]}},{"text":"%md\n### iii) Find the users who have rated at least 50 movies and identify their favourite movie genres.","user":"anonymous","dateUpdated":"2025-07-23T22:42:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>iii) Find the users who have rated at least 50 movies and identify their favourite movie genres.</h3>\n"}]},"apps":[],"jobName":"paragraph_1753226801679_-71814699","id":"20250722-232641_1989424554","dateCreated":"2025-07-22T23:26:41+0000","dateStarted":"2025-07-23T22:42:13+0000","dateFinished":"2025-07-23T22:42:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5332"},{"text":"%pyspark\n\n# import necessary PySpark libraries\nfrom pyspark.sql.functions import col, count, row_number\nfrom pyspark.sql.window import Window\n\n# filter users who have rated at least 50 movies\nfilter_users = readRatings.groupBy(\"user_id\") \\\n    .agg(count(\"*\").alias(\"num_ratings\")) \\\n    .filter(col(\"num_ratings\") >= 50) \\\n    .select(\"user_id\")\n\n# join ratings with filtered users and movie metadata\nfilter_ratings = readRatings \\\n    .join(filtered_users, on=\"user_id\") \\\n    .join(readMovies, on=\"movie_id\")\n\n# define list of genre columns to analyze\ngenres = [\n    \"unknown\", \"action\", \"adventure\", \"animation\", \"children\", \"comedy\", \"crime\",\n    \"documentary\", \"drama\", \"fantasy\", \"filmNoir\", \"horror\", \"musical\", \"mystery\",\n    \"romance\", \"sciFi\", \"thriller\", \"war\", \"western\"\n]\n\n# for each genre, create a dataframe of users who rated a movie of the genre\nuser_genres = None\nfor genre in genres:\n    temp = filter_ratings.filter(col(genre) == 1).select(\"user_id\").withColumn(\"genre\", F.lit(genre))\n    user_genres = temp if user_genres is None else user_genres.union(temp)\n    \n# count how many times each user rated movies of each genre\nuser_genre_counts = user_genres.groupBy(\"user_id\", \"genre\") \\\n    .agg(count(\"*\").alias(\"count\"))\n\n# rank genres per user by count\nwindowSpec = Window.partitionBy(\"user_id\").orderBy(col(\"count\").desc())\nranked_genres = user_genre_counts.withColumn(\"rank\", row_number().over(windowSpec))\n\n# select the top ranked genre for each user\ntop_genres_per_user = ranked_genres.filter(col(\"rank\") == 1) \\\n    .orderBy(col(\"count\").desc())\n\n# display the most frequent genre per user\ntop_genres_per_user.select(\"user_id\", \"genre\", \"count\").show()","user":"anonymous","dateUpdated":"2025-07-23T22:42:17+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1753307988278_-1126058896","id":"20250723-215948_952694620","dateCreated":"2025-07-23T21:59:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6896","dateFinished":"2025-07-23T22:43:05+0000","dateStarted":"2025-07-23T22:42:17+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+------+-----+\n|user_id| genre|count|\n+-------+------+-----+\n|    655| drama|  410|\n|    405| drama|  309|\n|    537| drama|  251|\n|    450| drama|  237|\n|     13| drama|  218|\n|    234| drama|  213|\n|    416| drama|  212|\n|    279|comedy|  211|\n|    201| drama|  196|\n|    393|comedy|  191|\n|    181| drama|  188|\n|    303|comedy|  184|\n|    334| drama|  174|\n|     90| drama|  173|\n|    378| drama|  170|\n|    276| drama|  168|\n|    429| drama|  167|\n|    474| drama|  166|\n|    293| drama|  165|\n|    308| drama|  163|\n+-------+------+-----+\nonly showing top 20 rows\n\n"}]}},{"text":"%md\n### iv) Find all the users who are less than 20 years old.","user":"anonymous","dateUpdated":"2025-07-23T22:42:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>iv) Find all the users who are less than 20 years old.</h3>\n"}]},"apps":[],"jobName":"paragraph_1753304971316_1738320642","id":"20250723-210931_2137416040","dateCreated":"2025-07-23T21:09:31+0000","dateStarted":"2025-07-23T22:42:13+0000","dateFinished":"2025-07-23T22:42:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5333"},{"text":"%pyspark\n\n# filter users who are younger than 20 years old\nuser_u20 = readUsers.filter(\"age < 20\")\n\n# display the filtered user records\nuser_u20.show()","user":"anonymous","dateUpdated":"2025-07-23T22:42:19+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1753308103151_-212094724","id":"20250723-220143_236606156","dateCreated":"2025-07-23T22:01:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6952","dateFinished":"2025-07-23T22:43:05+0000","dateStarted":"2025-07-23T22:42:19+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+---+------+----------+-----+\n|user_id|age|gender|occupation|  zip|\n+-------+---+------+----------+-----+\n|    110| 19|     M|   student|77840|\n|    849| 15|     F|   student|25652|\n|    729| 19|     M|   student|56567|\n|    631| 18|     F|   student|38866|\n|    787| 18|     F|   student|98620|\n|    646| 17|     F|   student|51250|\n|    925| 18|     F|  salesman|49036|\n|    619| 17|     M|   student|44134|\n|    320| 19|     M|   student|24060|\n|    461| 15|     M|   student|98102|\n|     67| 17|     M|   student|60402|\n|    482| 18|     F|   student|40256|\n|    303| 19|     M|   student|14853|\n|    101| 15|     M|   student|05146|\n|    872| 19|     F|   student|74078|\n|    601| 19|     F|    artist|99687|\n|    817| 19|     M|   student|60152|\n|    710| 19|     M|   student|92020|\n|    246| 19|     M|   student|28734|\n|    451| 16|     M|   student|48446|\n+-------+---+------+----------+-----+\nonly showing top 20 rows\n\n"}]}},{"text":"%md\n### v) Find all the users whose occupation is “scientist” and whose age is between 30 and 40 years old.","user":"anonymous","dateUpdated":"2025-07-23T22:42:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>v) Find all the users whose occupation is “scientist” and whose age is between 30 and 40 years old.</h3>\n"}]},"apps":[],"jobName":"paragraph_1753304907791_1075890305","id":"20250723-210827_1834395484","dateCreated":"2025-07-23T21:08:27+0000","dateStarted":"2025-07-23T22:42:13+0000","dateFinished":"2025-07-23T22:42:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5334"},{"text":"%pyspark\n\n# import necessary PySpark library\nfrom pyspark.sql.functions import col\n\n# filter users who are scientists and aged between 30 and 40\nscientist_mid = readUsers.filter(\n    (col(\"age\") > 29) & (col(\"age\") < 41) & (col(\"occupation\") == \"scientist\"))\n    \n# display the filtered record\nscientist_mid.show()","user":"anonymous","dateUpdated":"2025-07-23T22:42:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1753304950871_-2135746682","id":"20250723-210910_1841126166","dateCreated":"2025-07-23T21:09:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5335","dateFinished":"2025-07-23T22:43:05+0000","dateStarted":"2025-07-23T22:43:05+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+---+------+----------+-----+\n|user_id|age|gender|occupation|  zip|\n+-------+---+------+----------+-----+\n|    874| 36|     M| scientist|37076|\n|    538| 31|     M| scientist|21010|\n|    272| 33|     M| scientist|53706|\n|    430| 38|     M| scientist|98199|\n|    643| 39|     M| scientist|55122|\n|    543| 33|     M| scientist|95123|\n|    554| 32|     M| scientist|62901|\n|     40| 38|     M| scientist|27514|\n|     71| 39|     M| scientist|98034|\n|     74| 39|     M| scientist|T8H1N|\n|    183| 33|     M| scientist|27708|\n|    107| 39|     M| scientist|60466|\n|    918| 40|     M| scientist|70116|\n|    337| 37|     M| scientist|10522|\n|    730| 31|     F| scientist|32114|\n|    309| 40|     M| scientist|70802|\n+-------+---+------+----------+-----+\n\n"}]}},{"text":"","user":"anonymous","dateUpdated":"2025-07-23T22:42:14+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1753308570375_846989303","id":"20250723-220930_1612851736","dateCreated":"2025-07-23T22:09:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7041"}],"name":"P152419_STQD6324_PROJECT3","id":"2KZ8R8BPE","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}